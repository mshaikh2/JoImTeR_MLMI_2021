{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision as tv\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, cosine_distances\n",
    "\n",
    "from model_mlm_cls import Text_Classification\n",
    "from transformers import BertConfig\n",
    "\n",
    "from misc.config import Config\n",
    "cfg  = Config()\n",
    "cfg.GPU_ID = 1\n",
    "\n",
    "torch.cuda.set_device(cfg.GPU_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DIM = 2048\n",
    "\n",
    "val_transform = tv.transforms.Compose([\n",
    "    tv.transforms.CenterCrop(MAX_DIM),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "\n",
    "class IUDataset(Dataset):\n",
    "    def __init__(self, root, dataset, max_length, transform=val_transform, mode='test'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root  #save dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.classes = dataset['classes']  # multi-label list\n",
    "        self.datadict = dataset['data_dict']  # uid: {text:text, filenames:[filename]}\n",
    "        if self.mode == 'test':\n",
    "            self.keys = np.concatenate([dataset['data_split']['train_uids'], dataset['data_split']['val_uids'],\n",
    "                                        dataset['data_split']['test_uids']])  # uid list\n",
    "\n",
    "        #         self.idx2word = dataset['idx2word']\n",
    "        #         self.idx2word[8410] = '[MASK]'\n",
    "        #         self.word2idx = dataset['word2idx']\n",
    "        #         self.word2idx['[MASK]'] = 8410\n",
    "        #         self.__sep_id__ = dataset['word2idx']['[SEP]']\n",
    "        self.vocab_size = 28996\n",
    "        self.max_length = max_length + 1\n",
    "        #         self.__mask_id__ = 8410 # [MASK] token id\n",
    "\n",
    "        #         ## classification params\n",
    "        #         self.class_to_idx = {\n",
    "        #             'Atelectasis': 0, 'Cardiomegaly': 1, 'Consolidation': 2, 'Edema': 3, 'Enlarged Cardiomediastinum': 4,\n",
    "        #             'Fracture': 5, 'Lung Lesion': 6, 'Lung Opacity': 7, 'No Finding': 8, 'Pleural Effusion': 9,\n",
    "        #             'Pleural Other': 10, 'Pneumonia': 11, 'Pneumothorax': 12, 'Support Devices': 13\n",
    "        #         }\n",
    "\n",
    "        #         self.idx_to_class = {\n",
    "        #             0:'Atelectasis', 1:'Cardiomegaly', 2:'Consolidation', 3:'Edema', 4:'Enlarged Cardiomediastinum',\n",
    "        #             5:'Fracture', 6:'Lung Lesion', 7:'Lung Opacity', 8:'No Finding', 9:'Pleural Effusion',\n",
    "        #             10:'Pleural Other', 11:'Pneumonia', 12:'Pneumothorax', 13:'Support Devices'\n",
    "        #         }\n",
    "\n",
    "        self.class_to_idx = {'no finding': 8\n",
    "            , 'edema': 3\n",
    "            , 'consolidation': 2\n",
    "            , 'pneumonia': 11\n",
    "            , 'pneumothorax': 12\n",
    "            , 'atelectasis': 0\n",
    "            , 'cardiomegaly': 1\n",
    "            , 'effusion': 9}\n",
    "\n",
    "        self.idx_to_class = {0: 'atelectasis'\n",
    "            , 1: 'cardiomegaly'\n",
    "            , 9: 'effusion'\n",
    "            , 3: 'edema'\n",
    "            , 2: 'consolidation'\n",
    "            , 11: 'pneumonia'\n",
    "            , 12: 'pneumothorax'\n",
    "            , 8: 'no finding'}\n",
    "\n",
    "        self.num_classes = 14\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.keys[idx]\n",
    "\n",
    "        #         image_id = np.random.choice(self.datadict[uid]['filenames'])# get one file name randomly\n",
    "        #         image_path = os.path.join(self.root, image_id) #original used 'jpg', try 'png'\n",
    "\n",
    "        #         try:\n",
    "        #             with Image.open(image_path) as img:\n",
    "        #                 if self.transform:\n",
    "        #                     image = self.transform(img)\n",
    "\n",
    "        #         except Exception as ex:\n",
    "        #             print(ex)\n",
    "        # #             with open(self.err_log, 'a+') as f:\n",
    "        # #                 f.write('%s\\nERR_IMG %s\\n' % (ex, image_path))\n",
    "        #             return None ## return None, collate_fn will ignore this broken sample\n",
    "\n",
    "        classes = torch.tensor([self.class_to_idx[x] for x in self.classes[uid]])\n",
    "        y_onehot = torch.FloatTensor(self.num_classes).zero_()\n",
    "        y_onehot.scatter_(0, classes, 1)\n",
    "\n",
    "        ## load text input ##\n",
    "        max_len_array = np.zeros(self.max_length, dtype='int')\n",
    "        cap_mask = np.zeros(self.max_length, dtype='int')\n",
    "        caption = np.array(self.datadict[uid]['cb_token_ids'])\n",
    "        if len(caption) <= self.max_length:\n",
    "            cap_mask[:len(caption)] = 1\n",
    "            max_len_array[:len(caption)] = caption\n",
    "        else:\n",
    "            cap_mask[:] = 1\n",
    "            max_len_array = caption[:self.max_length]\n",
    "            max_len_array[-1] = self.__sep_id__\n",
    "        #         caption = max_len_array\n",
    "        cap_mask = cap_mask.astype(bool)\n",
    "        cap_lens = cap_mask.sum(-1)\n",
    "\n",
    "        return max_len_array, cap_mask, y_onehot, uid, cap_lens\n",
    "\n",
    "\n",
    "def build_dataset(mode='test', cfg=None):\n",
    "    data_dir = cfg.dataset_root\n",
    "    img_dir = os.path.join(data_dir, 'images', 'images_normalized')\n",
    "    with open(os.path.join(data_dir, 'cleaned_dataset_v4.pickle'), 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    if mode == 'test':\n",
    "        data = IUDataset(img_dir, dataset,\n",
    "                         max_length=cfg.max_length, mode=mode,\n",
    "                         transform=val_transform)\n",
    "        return data\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{mode} not supported\")\n",
    "\n",
    "\n",
    "## collate_fn for handling None type item due to image corruption ##\n",
    "## This will return batch size - broken image number ##\n",
    "def collate_fn_ignore_none(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set 3666 is loaded.\n",
      "Vocab size is 28996.\n",
      "Max length is 160\n"
     ]
    }
   ],
   "source": [
    "# cfg.max_length = 127\n",
    "\n",
    "test_set = build_dataset('test', cfg)\n",
    "# test_set = build_dataset('test', cfg)\n",
    "print('Testing set %d is loaded.' % len(test_set))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=50,\n",
    "    collate_fn=collate_fn_ignore_none, drop_last=False,\n",
    "    shuffle=False, num_workers=8, pin_memory=False)\n",
    "print('Vocab size is %d.' % test_set.vocab_size)\n",
    "print('Max length is %d' % test_set.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert encoder with MaskedLMhead.\n",
      "Initiate text encoder from MLM pretrained parameters from: /media/My1TBSSD1/MICCAI2021/output/MIMIC-CXR_mlm_wordpiece_2021_03_25_19_20_18/Model/text_encoder.pth\n",
      "Load image encoder checkpoint from: /media/My1TBSSD1/MICCAI2021/output/MIMIC-CXR_mimic_mlm_cls_wp_2021_03_26_10_40_49/Model/Txt_class_model14.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_config = BertConfig(vocab_size=test_loader.dataset.vocab_size, hidden_size=512, num_hidden_layers=3,\n",
    "                         num_attention_heads=8, intermediate_size=2048, hidden_act='gelu',\n",
    "                         hidden_dropout_prob=cfg.hidden_dropout_prob,\n",
    "                         attention_probs_dropout_prob=cfg.attention_probs_dropout_prob,\n",
    "                         max_position_embeddings=512, layer_norm_eps=1e-12,\n",
    "                         initializer_range=0.02, type_vocab_size=2, pad_token_id=0)\n",
    "\n",
    "##### change the checkpoint path here #####\n",
    "# cfg.text_encoder_path = '/media/My1TBSSD1/IPMI2021/output/MIMIC_class_txt_ft_new_2021_03_03_01_29_36/Model/Txt_class_model16.pth'\n",
    "# cfg.text_encoder_path = '/media/My1TBSSD1/IPMI2021/output/MIMIC_class_mlm_cls_ft_2021_03_19_14_33_50/Model/Txt_class_model4.pth'\n",
    "cfg.text_encoder_path = '/media/My1TBSSD1/MICCAI2021/output/MIMIC-CXR_mimic_mlm_cls_wp_2021_03_26_10_40_49/Model/Txt_class_model14.pth'\n",
    "# ################### encoders ################################# #      \n",
    "image_encoder = Text_Classification(num_class=14, pretrained=False, cfg=cfg, bert_config=bert_config)\n",
    "\n",
    "if cfg.CUDA:\n",
    "    image_encoder = image_encoder.cuda()\n",
    "\n",
    "if cfg.text_encoder_path != '':\n",
    "    #     img_encoder_path = cfg.text_encoder_path.replace('text_encoder', 'image_encoder')\n",
    "    print('Load image encoder checkpoint from:', cfg.text_encoder_path)\n",
    "    state_dict = torch.load(cfg.text_encoder_path, map_location='cpu')\n",
    "    image_encoder.load_state_dict(state_dict['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(cnn_model, dataloader_val):\n",
    "    cnn_model.eval()\n",
    "    val_data_iter = iter(dataloader_val)\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    class_auc = []\n",
    "    #####################################\n",
    "    for step in tqdm(range(len(val_data_iter))):\n",
    "        captions, cap_masks, classes, uids, cap_lens = val_data_iter.next()\n",
    "        if cfg.CUDA:\n",
    "            captions, cap_masks, classes = captions.cuda(), cap_masks.cuda(), classes.cuda()\n",
    "\n",
    "        y_pred = cnn_model(captions, cap_masks)\n",
    "        y_pred_sigmoid = torch.sigmoid(y_pred)\n",
    "        y_preds.append(y_pred_sigmoid.detach().cpu().numpy())\n",
    "        y_trues.append(classes.detach().cpu().numpy())\n",
    "\n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "    for i in [0, 1, 2, 3, 8, 9, 11, 12]:\n",
    "        class_auc.append(roc_auc_score(y_trues[:, i], y_preds[:, i]))\n",
    "\n",
    "    return class_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:02<00:00, 30.37it/s]\n"
     ]
    }
   ],
   "source": [
    "auc = evaluate(image_encoder, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atelectasis: 0.9274\n",
      "cardiomegaly: 0.9041\n",
      "consolidation: 0.9152\n",
      "edema: 0.9610\n",
      "no finding: 0.8583\n",
      "effusion: 0.9498\n",
      "pneumonia: 0.8650\n",
      "pneumothorax: 0.9819\n",
      "Avg: 0.9292\n",
      "wAvg: 0.9228\n"
     ]
    }
   ],
   "source": [
    "openi_cls = [0, 1, 2, 3, 8, 9, 11, 12]\n",
    "# auc = np.array(auc)[[0,1,2,3,5,6,7]]\n",
    "\n",
    "for idx in range(len(auc)):\n",
    "    print('%s: %.4f' % (test_loader.dataset.idx_to_class[openi_cls[idx]], auc[idx]))\n",
    "\n",
    "avg = np.mean(np.array(auc)[[0, 1, 2, 3, 5, 6, 7]])\n",
    "print('Avg: %.4f' % avg)\n",
    "\n",
    "# weight = np.array([295, 319, 28, 41, 2988, 141, 36, 25]) # weight for the whole open-i dataset\n",
    "weight = np.array([295, 319, 28, 41, 141, 36, 25])  # weight for the whole open-i dataset\n",
    "weight = weight / weight.sum()\n",
    "wavg = np.array(auc)[[0, 1, 2, 3, 5, 6, 7]] @ weight\n",
    "print('wAvg: %.4f' % wavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atelectasis: 0.9416\n",
      "cardiomegaly: 0.9030\n",
      "consolidation: 0.9456\n",
      "edema: 0.9869\n",
      "no finding: 0.8667\n",
      "effusion: 0.9716\n",
      "pneumonia: 0.8921\n",
      "pneumothorax: 0.9842\n",
      "Avg: 0.9464\n",
      "wAvg: 0.9339\n"
     ]
    }
   ],
   "source": [
    "openi_cls = [0, 1, 2, 3, 8, 9, 11, 12]\n",
    "# auc = np.array(auc)[[0,1,2,3,5,6,7]]\n",
    "\n",
    "for idx in range(len(auc)):\n",
    "    print('%s: %.4f' % (test_loader.dataset.idx_to_class[openi_cls[idx]], auc[idx]))\n",
    "\n",
    "avg = np.mean(np.array(auc)[[0, 1, 2, 3, 5, 6, 7]])\n",
    "print('Avg: %.4f' % avg)\n",
    "\n",
    "# weight = np.array([295, 319, 28, 41, 2988, 141, 36, 25]) # weight for the whole open-i dataset\n",
    "weight = np.array([295, 319, 28, 41, 141, 36, 25])  # weight for the whole open-i dataset\n",
    "weight = weight / weight.sum()\n",
    "wavg = np.array(auc)[[0, 1, 2, 3, 5, 6, 7]] @ weight\n",
    "print('wAvg: %.4f' % wavg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[958, 997, 233, 600, 174, 79, 168, 1134, 1636, 1243, 31, 361, 234, 1566]\n"
     ]
    }
   ],
   "source": [
    "## statistic for val sets and test sets\n",
    "hold = np.array([679, 808, 191, 659, 132, 78, 108, 974, 539, 990, 52, 309, 94, 1061])\n",
    "val1 = np.array([392, 422, 102, 246, 64, 46, 63, 484, 619, 533, 19, 134, 110, 685])\n",
    "val2 = np.array([566, 575, 131, 354, 110, 33, 105, 650, 1017, 710, 12, 227, 124, 881])\n",
    "print(list((val1+val2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566\n",
      "575\n",
      "131\n",
      "354\n",
      "110\n",
      "33\n",
      "105\n",
      "650\n",
      "1017\n",
      "710\n",
      "12\n",
      "227\n",
      "124\n",
      "881\n"
     ]
    }
   ],
   "source": [
    "for i in val2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}